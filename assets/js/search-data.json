{
  
    
        "post0": {
            "title": "KVN, Margaret & bingo - Part 2",
            "content": "In Part 1, we defined what we are trying to achieve (KVN whispering Margaret the drawn numbers) and what our proposed solution (a Neural Network). . Learning time! . Now we have an idea of what the new system we want to implement to KVN will look like. The problem is we need to be able to train this system. That is, we need a method for KVN to learn to identify the numbers using this system. . In more mathematical terms, learning means finding the optimal parameter set of weights $w$ and biases $b$ such that, at the end, KVN gets the digit right the maximum possible number of times. This is actually a big optimization problem and the concepts used in smaller problems hold. . The main idea required so that KVN can learn is that he needs to understand when he is wrong and how wrong he is. Once this is defined, he can learn to be less wrong the next time he tries (i.e. the parameters can be optimized). Ideally, if he tries a lot of times, he will eventually be as right as he can be with the particular system we design for him. . Wrongness meter (a.k.a Loss Function) . So, how can we define how wrong is KVN? The idea is to define what it is called a loss function or cost function $C$. This function must represent the distance between the truth (i.e. the correct digit) and the system&#39;s output (i.e. KVN&#39;s guess). That is, how wrong is KVN. . The two natural conditions that this function has to fulfill is that it has to depend on the output (otherwise it wouldn&#39;t make sense) and it has to be strictly positive (we will see in a moment why). Again, as in the activation function case, there are many different cost functions with different properties (but we won&#39;t enter into more detail here). Two of the most popular cost functions are: . The quadratic cost function: | . $$ C(w,b) = frac{1}{2n} sum_i | o(i) - a(i) |^2 $$ . This function represent the mean quadratic distance over all inputs $i$ (all images to learn) between the output corresponding to the input (guess of each image) and the correct answer $a$. It solely depends on the system parameters $w, b$. $n$ is the total number of inputs. This is the traditionally used cost function in optimization (or regression) problems. . The cross entropy cost function: | . $$ C(w,b) = - frac{1}{n} sum_i [o ln{a}+ (1-o) ln{(1-a)}] $$ . Notice that in this case, abusing notation, $a,o$ are equivalent to $a(i), o(i)$. This function is designed to solve some of the limitations of the quadratic cost function. In fact, cross-entropy is widely used in problems like the one we are trying to solve. . Now that KVN have a wrongness meter, how can we he use it to learn anything? . The downhill bike (a.k.a. Gradient Descent) . The typical method to solve optimization problems is what it is called gradient descent. This might sound quite complicated but the main idea is rather simple. To illustrate, let&#39;s define an arbitrary cost function that depends only on a single variable $x$. The situation is the following: . #collapse-hide import numpy as np import matplotlib.pyplot as plt # Define x grid. xx = np.linspace(-6,3,100) # Arbitrary cost function. def cost_fun(x): return 0.01*x**4 + 0.05*x**3 + 2 # Compute function shape. y = cost_fun(xx) fig = plt.figure(figsize=(6,2.4)) # Initial situation ax1 = plt.subplot(121) ax1.plot(xx, y, zorder=-1) ax1.scatter(2.5, cost_fun(2.5), color=&#39;r&#39;) ax1.arrow(2,3,-0.4,-0.4, width = 0.05, color=&#39;k&#39;) ax1.set_title(&quot;Initial situation&quot;) ax1.set_xlabel(&quot;$x$&quot;) ax1.set_ylabel(&quot;$C(x)$&quot;) # Final situation ax2 = plt.subplot(122) ax2.plot(xx, y, zorder=-1) ax2.scatter(xx[np.argmin(y)], min(y), color=&#39;lime&#39;) ax2.set_title(&quot;Final situation&quot;) ax2.set_xlabel(&quot;$x$&quot;) ax2.set_ylabel(&quot;$C(x)$&quot;); plt.tight_layout() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; The idea behind gradient descent is quite simple. Imagine that initially the KVN&#39;s system is quite wrong and we start in the leftmost situation. We compute the cost function and it happens that we are far away from the minimum, which represents the least possible error we can have with our system (that is why we needed a positive function as having negative errors doesn&#39;t make much sense). Qualitatively, we would like to go downhill until we reach the minimum of the cost function, as in the leftmost plot. Following this explanation, we can imagine gradient descent as downhill biking. . Nonetheless, there is no easy way to know if we are at the very minimum. What we can do is look at our hill and see if we are in a slope. If that is the case, we ride our bike downhill a little bit before checking again. If we repeat this procedure many many times, we will end up, ideally, in the rightmost situation: we are in the minimum (i.e. KVN gets it right the maximum number of times the system allows him) so the system is optimized. . Mathematically, the slope is the derivative of the cost function with respect to the parameter set evaluated at the current parameter set. Therefore, the idea behind gradient descent is that we iteratively compute the derivative and update our parameter set such that we get closer to the minimum. Hopefully, in the end, we will reach the global minimum. The expression used for this update is: . $$ v rightarrow v&#39; = v - eta nabla C$$ . Here, $v$ is the parameter set $w, b$ of the system. $ eta$ is what it is called the learning rate which is equivalent to how long are our rides with the bycicle between slope checks. If it is too small, we will never reach the valley. If it is too big, we will end up going from one side of the valley to the other one, never reaching the bottom. The $ nabla C$ is the gradient of the cost function, which is equivalent to the slope of the mountain. There is a $-$ sign because we always want to go down. Remember that we are downhill biking! . The next question one might ask is: . Okay... but how exactly do you determine the slope of a mountain with a hell lot of dimensions?! . Remember that we have a lot of parameters and each parameter corresponds to a dimension. Thus, we have as many dimensions as parameters. The example above have only one dimension. Thus, this is a totally valid question. How would you know where down is if you were in a world that has 1000 dimensions? (Our world have three spatial dimensions). . As we have seen, the slope corresponds to the gradient. In computational sciences, gradients are usually computed by what is called finite differences. We won&#39;t discuss that here but the idea is that it requires to compute what happens to the cost function if we vary by a small amount a single parameter. And repeat for all parameters. This means that in order to determine the slope at every check, we have to compute the cost function as many times as parameters we have (a lot). And remember that in order to evaluate the cost functions we saw above we have to evaluate all training examples. Although this works, it is extremely slow. And by extremely I mean prohibitively slow. In our biking situation, night would fall upon us and no one wants to bike at night, for night is dark and full of terrors. . One way to speed things up is what is called stochastic gradient descent. The idea is that instead of using all input examples at each step, we use a random subsample of them, a mini-batch at each slope check. Then, the cost function is computed for this subsample and used to determine the slope. In the next check, a different subsample is used. This procedure is repeated until all training samples have been used which is said to complete an epoch of training. After each epoch, the procedure is started again. . Using this method, we only get an idea of where down is at each check, but an idea is sufficient to advance a bit. It is like our sense of equilibrium is a bit distorted but we always know more or less where down is. That way, KVN only has to look at a fraction of the images to learn a bit and is faster in dismounting, checking and riding the bike again. The path that KVN will follow with stochastic gradient descent will be less direct but the overall ride will also be much faster. Altough this method can improve the speed, the slope check is still too slow. . The slopemeter (a.k.a. Backpropagation) . The method usually used to compute the slope is the backpropagation algorithm. The idea is that using the chain rule of basic calculus, we can compute which effect a small perturbation of a parameter will have in the final cost function. These effects are actually the derivatives we were talking before, so what we are saying is that we use the chain rule to obtain simple forms to compute the derivatives. The name backpropagation comes from the fact that the algorithm starts at the output, and from the error in the cost function backpropagates through the network computing these perturbations and thus, obtaining the derivatives. . In the end, this means that each slope check, the network has to be used once per training sample (or image) in the mini-batch in the forward direction to compute the cost function (check how wrong KVN is in each image) and once per sample in the backward direction to compute the gradient (how different would be the error if some parameter was different). In the end, this method is much much faster than finite differences and allow neural networks to be really feasible. . My dear readers, we have finally revised all the building blocks required to understand the system we intend to implement into KVN for him to recognize the bingo digits. Time to get the job done! In the next part, we will finally teach KVN what it needs to know! .",
            "url": "https://bepuca.dev/machine%20learning/2020/06/04/kvn-part-2.html",
            "relUrl": "/machine%20learning/2020/06/04/kvn-part-2.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "KVN, Margaret & bingo - Part 2",
            "content": "In Part 1, we defined what we are trying to achieve (KVN whispering Margaret the drawn numbers) and what our proposed solution (a Neural Network). . Learning time! . Now we have an idea of what the new system we want to implement to KVN will look like. The problem is we need to be able to train this system. That is, we need a method for KVN to learn to identify the numbers using this system. . In more mathematical terms, learning means finding the optimal parameter set of weights $w$ and biases $b$ such that, at the end, KVN gets the digit right the maximum possible number of times. This is actually a big optimization problem and the concepts used in smaller problems hold. . The main idea required so that KVN can learn is that he needs to understand when he is wrong and how wrong he is. Once this is defined, he can learn to be less wrong the next time he tries (i.e. the parameters can be optimized). Ideally, if he tries a lot of times, he will eventually be as right as he can be with the particular system we design for him. . Wrongness meter (a.k.a Loss Function) . So, how can we define how wrong is KVN? The idea is to define what it is called a loss function or cost function $C$. This function must represent the distance between the truth (i.e. the correct digit) and the system&#39;s output (i.e. KVN&#39;s guess). That is, how wrong is KVN. . The two natural conditions that this function has to fulfill is that it has to depend on the output (otherwise it wouldn&#39;t make sense) and it has to be strictly positive (we will see in a moment why). Again, as in the activation function case, there are many different cost functions with different properties (but we won&#39;t enter into more detail here). Two of the most popular cost functions are: . The quadratic cost function: | . $$ C(w,b) = frac{1}{2n} sum_i | o(i) - a(i) |^2 $$ . This function represent the mean quadratic distance over all inputs $i$ (all images to learn) between the output corresponding to the input (guess of each image) and the correct answer $a$. It solely depends on the system parameters $w, b$. $n$ is the total number of inputs. This is the traditionally used cost function in optimization (or regression) problems. . The cross entropy cost function: | . $$ C(w,b) = - frac{1}{n} sum_i [o ln{a}+ (1-o) ln{(1-a)}] $$ . Notice that in this case, abusing notation, $a,o$ are equivalent to $a(i), o(i)$. This function is designed to solve some of the limitations of the quadratic cost function. In fact, cross-entropy is widely used in problems like the one we are trying to solve. . Now that KVN have a wrongness meter, how can we he use it to learn anything? . The downhill bike (a.k.a. Gradient Descent) . The typical method to solve optimization problems is what it is called gradient descent. This might sound quite complicated but the main idea is rather simple. To illustrate, let&#39;s define an arbitrary cost function that depends only on a single variable $x$. The situation is the following: . #collapse-hide import numpy as np import matplotlib.pyplot as plt # Define x grid. xx = np.linspace(-6,3,100) # Arbitrary cost function. def cost_fun(x): return 0.01*x**4 + 0.05*x**3 + 2 # Compute function shape. y = cost_fun(xx) fig = plt.figure(figsize=(6,2.4)) # Initial situation ax1 = plt.subplot(121) ax1.plot(xx, y, zorder=-1) ax1.scatter(2.5, cost_fun(2.5), color=&#39;r&#39;) ax1.arrow(2,3,-0.4,-0.4, width = 0.05, color=&#39;k&#39;) ax1.set_title(&quot;Initial situation&quot;) ax1.set_xlabel(&quot;$x$&quot;) ax1.set_ylabel(&quot;$C(x)$&quot;) # Final situation ax2 = plt.subplot(122) ax2.plot(xx, y, zorder=-1) ax2.scatter(xx[np.argmin(y)], min(y), color=&#39;lime&#39;) ax2.set_title(&quot;Final situation&quot;) ax2.set_xlabel(&quot;$x$&quot;) ax2.set_ylabel(&quot;$C(x)$&quot;); plt.tight_layout() . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; The idea behind gradient descent is quite simple. Imagine that initially the KVN&#39;s system is quite wrong and we start in the leftmost situation. We compute the cost function and it happens that we are far away from the minimum, which represents the least possible error we can have with our system (that is why we needed a positive function as having negative errors doesn&#39;t make much sense). Qualitatively, we would like to go downhill until we reach the minimum of the cost function, as in the leftmost plot. Following this explanation, we can imagine gradient descent as downhill biking. . Nonetheless, there is no easy way to know if we are at the very minimum. What we can do is look at our hill and see if we are in a slope. If that is the case, we ride our bike downhill a little bit before checking again. If we repeat this procedure many many times, we will end up, ideally, in the rightmost situation: we are in the minimum (i.e. KVN gets it right the maximum number of times the system allows him) so the system is optimized. . Mathematically, the slope is the derivative of the cost function with respect to the parameter set evaluated at the current parameter set. Therefore, the idea behind gradient descent is that we iteratively compute the derivative and update our parameter set such that we get closer to the minimum. Hopefully, in the end, we will reach the global minimum. The expression used for this update is: . $$ v rightarrow v&#39; = v - eta nabla C$$ . Here, $v$ is the parameter set $w, b$ of the system. $ eta$ is what it is called the learning rate which is equivalent to how long are our rides with the bycicle between slope checks. If it is too small, we will never reach the valley. If it is too big, we will end up going from one side of the valley to the other one, never reaching the bottom. The $ nabla C$ is the gradient of the cost function, which is equivalent to the slope of the mountain. There is a $-$ sign because we always want to go down. Remember that we are downhill biking! . The next question one might ask is: . Okay... but how exactly do you determine the slope of a mountain with a hell lot of dimensions?! . Remember that we have a lot of parameters and each parameter corresponds to a dimension. Thus, we have as many dimensions as parameters. The example above have only one dimension. Thus, this is a totally valid question. How would you know where down is if you were in a world that has 1000 dimensions? (Our world have three spatial dimensions). . As we have seen, the slope corresponds to the gradient. In computational sciences, gradients are usually computed by what is called finite differences. We won&#39;t discuss that here but the idea is that it requires to compute what happens to the cost function if we vary by a small amount a single parameter. And repeat for all parameters. This means that in order to determine the slope at every check, we have to compute the cost function as many times as parameters we have (a lot). And remember that in order to evaluate the cost functions we saw above we have to evaluate all training examples. Although this works, it is extremely slow. And by extremely I mean prohibitively slow. In our biking situation, night would fall upon us and no one wants to bike at night, for night is dark and full of terrors. . One way to speed things up is what is called stochastic gradient descent. The idea is that instead of using all input examples at each step, we use a random subsample of them, a mini-batch at each slope check. Then, the cost function is computed for this subsample and used to determine the slope. In the next check, a different subsample is used. This procedure is repeated until all training samples have been used which is said to complete an epoch of training. After each epoch, the procedure is started again. . Using this method, we only get an idea of where down is at each check, but an idea is sufficient to advance a bit. It is like our sense of equilibrium is a bit distorted but we always know more or less where down is. That way, KVN only has to look at a fraction of the images to learn a bit and is faster in dismounting, checking and riding the bike again. The path that KVN will follow with stochastic gradient descent will be less direct but the overall ride will also be much faster. Altough this method can improve the speed, the slope check is still too slow. . The slopemeter (a.k.a. Backpropagation) . The method usually used to compute the slope is the backpropagation algorithm. The idea is that using the chain rule of basic calculus, we can compute which effect a small perturbation of a parameter will have in the final cost function. These effects are actually the derivatives we were talking before, so what we are saying is that we use the chain rule to obtain simple forms to compute the derivatives. The name backpropagation comes from the fact that the algorithm starts at the output, and from the error in the cost function backpropagates through the network computing these perturbations and thus, obtaining the derivatives. . In the end, this means that each slope check, the network has to be used once per training sample (or image) in the mini-batch in the forward direction to compute the cost function (check how wrong KVN is in each image) and once per sample in the backward direction to compute the gradient (how different would be the error if some parameter was different). In the end, this method is much much faster than finite differences and allow neural networks to be really feasible. . My dear readers, we have finally revised all the building blocks required to understand the system we intend to implement into KVN for him to recognize the bingo digits. Time to get the job done! In the next part, we will finally teach KVN what it needs to know! .",
            "url": "https://bepuca.dev/machine%20learning/2020/06/01/kvn-part-2.html",
            "relUrl": "/machine%20learning/2020/06/01/kvn-part-2.html",
            "date": " • Jun 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "KVN, Margaret & bingo - Part 1",
            "content": "Neural Networks (NNs) are truly a hot topic these days. Or at least, the so-called Artificial Intelligence is. Nonetheless, I realized that, for most of my friends outside the machine learning niche, Neural Networks seem like dark magic (at least for the ones that care enough to ask). Here, I want to present an entertaining and illustrating narrative on what NNs are, how they work and an example of a problem that can (and actually is) solved by the technology. . Note: You will find here some of the mathematics and technical concepts. I introduce them for rigurosity and also to allow interested readers to delve deeper. I tried to make it approachable no matter what. So just skip whatever feels too technical and you will also understand the concepts, hopefully. If, despite that, you still have questions, please ask them in the comments! . Our very realistic problem . Imagine we are part of the ABC, the Amazing Bingo Club. This year it&#39;s the 100th anniversary of the club and a big event is coming up. We will have the biggest bingo) game the ABC has ever seen, for all the 29 members will be present! Moreover, we want to honor our beloved founder Margaret by letting her draw and call the numbers. But there is a problem. Margaret is blind now. . Let me say it again: Margaret. is. blind. :scream: . Fear not! We may have a solution. One of the members of the club has a KVN (pronounced Kevin), from the Final Space (much recommended series) universe. Meet KVN: . . KVN is a robot usually employed as a Deep Space Insanity Avoidance Companion. Now, we want to use it to read the numbers that Margaret draws and whisper them to her so she can then call the numbers aloud. The situation here is that KVN doesn&#39;t have a number recognition system implemented. That is, KVN can see but he is not able to recognize numbers... The good news are that we have access to KVN&#39;s main system, so if we are skilled enough, we could implement a digit recognizer into him. . But there is yet another problem (and we hope it to be the last). The ABC is in a really low budget so the numbers that Margaret will draw are handwritten numbers in small pieces of paper. Thus, we cannot just make KVN recognize some standard digits. Moreover, in the 100 years of the club, some numbers have been lost and replaced by many different members. So, in the end, we have a lot of different number styles within our bingo set. . In summary, then, we would like for KVN to be able to read handwritten digits similar to the ones below and whisper to Margaret: . It&#39;s a 2... . . Yeah, right, everything sounds pretty cool but how do we do it? . Well, one might think in the possibility to hardcode inside KVN&#39;s brain some rules to identify numbers such as the 2&#39;s have a semicircle on top, probably a loop on the bottom left, etc... As one can already divise, this is a ton of work for not so promising results. If that&#39;s not enough, it is also not that straightforward how to define such rules. . But what if we could show KVN a lot of examples of numbers so he can create his own rules to recognize them? What if we could do something like... . Us: Look KVN, this is a 2, this is also a 2, this is a 1, this is an 8, this is also an 8, etc. . | Us: *Showing him a new 2* What is this? . | KVN: A 2!!!! . | . Here is when Neural Networks (NN) enter the game. NNs are a tool that will allow KVN to learn to recognize digits as described above. . Note: What we are talking here is supervised learning of image classification, in the sense that we have labeled examples to teach KVN (e.g. images of 2 that we know are a 2) and the goal is to identify which number the image represents. NNs are used in a myriad of problems, not solely in supervised learning nor image classification. . First things first . When we talk about Neural Networks, we are actually talking about Artifical Neural Networks (ANNs). ANNs are vaguely inspired by the Biological Neural Networks (BNNs). BNNs or neural circuits are groups of interconnected neurons, a type of cell of the nervous system (including the brain). That is, your brain is full of BNN (among other things). . Neurons . These cells called neurons constitute a great part of our brain. Here a neuron: . . Their function, extremely simplified, is to transmit information. The way they work is the following: . A neuron receives a series of inputs from other neurons. | If the sum of these inputs is above a certain threshold, the neuron triggers and emits an output (that will go to other neurons). | More or less, we have 100 billion of these in our brain, which is A LOT. In principle, thanks to them we are able to sense and think but we will not delve deep into this. . The main idea we are interested in is: if we can learn with our brains which are constituted by neurons which are heavily interconnected... can we program artificial neurons, interconnect them and obtain an artificial brain that does something similar? Well, this is exactly what ANNs (NNs from now on) try to do. . Artificial neurons . In order to mimic the behaviour of neurons, artifical neurons are usually modeled as follows (image source): . . As we can see, these neurons have a series of inputs $i_i$, a weight $w_i$ associated with each input, an activation function $ sigma$ and an output $o$. . This might seem a bit obscure so let&#39;s try to clarify it a bit. The typical example used to explain artificial neurons is the following: Imagine there is a concert of our favourite group on Saturday and we want to decide whether or not to attend it. In order to take this decision, we consider several factors: . Will someone come with me? | Will it be good weather? | Can I go there by public transportation? | . To all these questions, which are my inputs $i_i$, affirmative answers (i.e. $1$) are good. But are they equally important? It might be the case that I don&#39;t really care if I have to go alone but I cannot withstand bad weather. How do I represent this? The weights $w_i$ are exactly for that. Bigger $w_i$ means bigger importance. Then, the activation function $ sigma$ is my decision function. A very simple function would be: . $$ o = sigma(i) = begin{cases} 1,&amp; text{if } sum_i w_i i_i gt threshold 0 ,&amp; text{if } sum_i w_i i_i lt threshold end{cases} $$That is, if the weighted input $ sum_i w_i i_i$ is greater than a threshold, the output $o$ will be $o=1$, I will go to the concert. In other words, only when the conditions I am considering to go to the concert are good enough for me I will attend the event. . In the neuron model, we can see that we have $ Sigma | sigma$. This is because the illustrative case is to use as an activation function the one explained above, which is simply the sum $ Sigma$ of the weighted inputs. But in general, the activation function used are a little bit more complicated than that. The idea is to introduce non-linearity, but it will stay out of the scope of this series. A general expression for these artificial neurons is: . $$ o = sigma big( sum_i w_i i_i + b big) $$ . Notice that a $b$, called bias, has appeared. This is equivalent to the threshold we were talking before, just a little rearranged. It can be thought as how difficult is to trigger the neuron. . There are many activation functions. The most popular are the following: . Sigmoid function: | . $$ sigma(x) = frac{1}{1+e^{-x}} $$ . Rectifier function (sometimes called ReLU): | . $$ sigma(x) = max(0,x) $$ . And their shape: . #collapse-hide import numpy as np import matplotlib.pyplot as plt # Define x grid. xx = np.linspace(-4,4,100) # Define response from the different activation functions. y_sig = 1/(1+np.exp(-xx)) y_relu = [max(0,x) for x in xx] # Plot the activation functions. plt.figure(figsize=(5,2)) # Sigmoid ax1 = plt.subplot(121) ax1.plot(xx,y_sig) ax1.set_title(&quot;Sigmoid&quot;) ax1.set_xlabel(&quot;$x$&quot;) ax1.set_ylabel(&quot;$ sigma(x)$&quot;) ax1.grid(alpha=0.3) # ReLU ax2 = plt.subplot(122) ax2.plot(xx,y_relu) ax2.set_title(&quot;ReLU&quot;) ax2.set_xlabel(&quot;$x$&quot;) ax2.set_ylabel(&quot;$ sigma(x)$&quot;) ax2.grid(alpha=0.3) plt.tight_layout(); . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Neural Networks . Now that we have our neurons, we can build our neural networks by interconnecting them. This would be a simple of example of such networkNow that we have our neurons, we can build our neural networks by interconnecting them. This is simple of example of such network: . . This seems a bit more complicated but it&#39;s actually the same idea with just more neurons. The input layer is basically a representation of the input values (there are no neurons in the input layer). The output layer are the artifical neurons which produce the final outputs. The hidden layer, basically a layer which is not an input nor an output, is constituted by several neurons. Each neuron has its inputs, its outputs and its activation function. If we talk about parameters, each neuron has a set of weights $w$ (one for each input $i$ it receives) and a bias $b$. We can already see that for this very little example we already have a lot of parameters. This will be important later on. . This small &quot;brain&quot; works as following: there are a series of inputs that go to the hidden layer, each neuron in this layer produces an output that becomes an input for the next layer (the output layer in this case) and finally, the output layer neurons produce the final outputs. . Returning to our original problem, we want to implement one of this Neural Networks inside KVN such that when we give him an image of a number as an input, he will then whisper which number it is as an output. . Note: This example is what it is called a feedforward neural network. In these networks, there are no cycles. All information always goes forward from input to output. There are other types of neural networks with different properties, such as recurrent neural networks, but they are more complicated so we will stick to the &quot;easy&quot; case. . So far, we have defined the mini-brain we will implement into KVN. In the following parts (coming soon), we will explore how Neural Networks can learn a task and, ultimately, do what we want them to do. . Part 2: The training building blocks: loss functions, gradient descent and backpropagation. | .",
            "url": "https://bepuca.dev/machine%20learning/2020/05/24/kvn-part-1.html",
            "relUrl": "/machine%20learning/2020/05/24/kvn-part-1.html",
            "date": " • May 24, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "You can contact me at bernatpuig [at] gmail [dot] com . I am Bernat and I work at the intersection of data science and software engineering. Mine has been (and it is still is) a journey of beautiful discoveries. I started in physics and eventually transitioned to data science and software development. Today, I strive to embed machine learning into software solutions to help people and organizations alike focus on what matters. I thrive the most when working within diverse and multidisciplinary teams and I believe in the value of Lean and XP software principles. . Curiosity driven, virtually everything interests me. This has exposed me to a myriad of technologies, frameworks and tools and it has also led me into a never-ending quest for learning and personal growth. I always enjoy engaging discussions, so please reach out to me if you want to talk about anything you find interesting. . Also, I like to ski. Ah, I love skiing. . .",
          "url": "https://bepuca.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bepuca.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}