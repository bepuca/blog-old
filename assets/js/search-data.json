{
  
    
        "post0": {
            "title": "KVN, Margaret & bingo - Part 1",
            "content": "Neural Networks (NNs) are truly a hot topic these days. Or at least, the so-called Artificial Intelligence is. Nonetheless, I realized that, for most of my friends outside the machine learning niche, Neural Networks seem like dark magic (at least for the ones that care enough to ask). Here, I want to present an entertaining and illustrating narrative on what NNs are, how they work and an example of a problem that can (and actually is) solved by the technology. . Our very realistic problem . Imagine we are part of the ABC, the Amazing Bingo Club. This year it&#39;s the 100th anniversary of the club and a big event is coming up. We will have the biggest bingo) game the ABC has ever seen, for all the 29 members will be present! Moreover, we want to honor our beloved founder Margaret by letting her draw and call the numbers. But there is a problem. Margaret is blind now. . Let me say it again: Margaret. is. blind. :scream: . Fear not! We may have a solution. One of the members of the club has a KVN (pronounced Kevin), from the Final Space (much recommended series) universe. Meet KVN: . . KVN is a robot usually employed as a Deep Space Insanity Avoidance Companion. Now, we want to use it to read the numbers that Margaret draws and whisper them to her so she can then call the numbers aloud. The situation here is that KVN doesn&#39;t have a number recognition system implemented. That is, KVN can see but he is not able to recognize numbers... The good news are that we have access to KVN&#39;s main system, so if we are skilled enough, we could implement a digit recognizer into him. . But there is yet another problem (and we hope it to be the last). The ABC is in a really low budget so the numbers that Margaret will draw are handwritten numbers in small pieces of paper. Thus, we cannot just make KVN recognize some standard digits. Moreover, in the 100 years of the club, some numbers have been lost and replaced by many different members. So, in the end, we have a lot of different number styles within our bingo set. . In summary, then, we would like for KVN to be able to read handwritten digits similar to the ones below and whisper to Margaret: . It&#39;s a 2... . . Yeah, right, everything sounds pretty cool but how do we do it? . Well, one might think in the possibility to hardcode inside KVN&#39;s brain some rules to identify numbers such as the 2&#39;s have a semicircle on top, probably a loop on the bottom left, etc... As one can already divise, this is a ton of work for not so promising results. If that&#39;s not enough, it is also not that straightforward how to define such rules. . But what if we could show KVN a lot of examples of numbers so he can create his own rules to recognize them? What if we could do something like... . Us: Look KVN, this is a 2, this is also a 2, this is a 1, this is an 8, this is also an 8, etc. . | Us: *Showing him a new 2* What is this? . | KVN: A 2!!!! . | . Here is when Neural Networks (NN) enter the game. NNs are a tool that will allow KVN to learn to recognize digits as described above. . Note: What we are talking here is supervised learning of image classification, in the sense that we have labeled examples to teach KVN (e.g. images of 2 that we know are a 2) and the goal is to identify which number the image represents. NNs are used in a myriad of problems, not solely in supervised learning nor image classification. . First things first . When we talk about Neural Networks, we are actually talking about Artifical Neural Networks (ANNs). ANNs are vaguely inspired by the Biological Neural Networks (BNNs). BNNs or neural circuits are groups of interconnected neurons, a type of cell of the nervous system (including the brain). That is, your brain is full of BNN (among other things). . Neurons . These cells called neurons constitute a great part of our brain. Here a neuron: . . Their function, extremely simplified, is to transmit information. The way they work is the following: . A neuron receives a series of inputs from other neurons. | If the sum of these inputs is above a certain threshold, the neuron triggers and emits an output (that will go to other neurons). | More or less, we have 100 billion of these in our brain, which is A LOT. In principle, thanks to them we are able to sense and think but we will not delve deep into this. . The main idea we are interested in is: if we can learn with our brains which are constituted by neurons which are heavily interconnected... can we program artificial neurons, interconnect them and obtain an artificial brain that does something similar? Well, this is exactly what ANNs (NNs from now on) try to do. . Artificial neurons . In order to mimic the behaviour of neurons, artifical neurons are usually modeled as follows (image source): . . As we can see, these neurons have a series of inputs $i_i$, a weight $w_i$ associated with each input, an activation function $ sigma$ and an output $o$. . This might seem a bit obscure so let&#39;s try to clarify it a bit. The typical example used to explain artificial neurons is the following: Imagine there is a concert of our favourite group on Saturday and we want to decide whether or not to attend it. In order to take this decision, we consider several factors: . Will someone come with me? | Will it be good weather? | Can I go there by public transportation? | . To all these questions, which are my inputs $i_i$, affirmative answers (i.e. $1$) are good. But are they equally important? It might be the case that I don&#39;t really care if I have to go alone but I cannot withstand bad weather. How do I represent this? The weights $w_i$ are exactly for that. Bigger $w_i$ means bigger importance. Then, the activation function $ sigma$ is my decision function. A very simple function would be: . $$ o = sigma(i) = begin{cases} 1,&amp; text{if } sum_i w_i i_i gt threshold 0 ,&amp; text{if } sum_i w_i i_i lt threshold end{cases} $$That is, if the weighted input $ sum_i w_i i_i$ is greater than a threshold, the output $o$ will be $o=1$, I will go to the concert. In other words, only when the conditions I am considering to go to the concert are good enough for me I will attend the event. . In the neuron model, we can see that we have $ Sigma | sigma$. This is because the illustrative case is to use as an activation function the one explained above, which is simply the sum $ Sigma$ of the weighted inputs. But in general, the activation function used are a little bit more complicated than that. The idea is to introduce non-linearity, but it will stay out of the scope of this series. A general expression for these artificial neurons is: . $$ o = sigma big( sum_i w_i i_i + b big) $$ . Notice that a $b$, called bias, has appeared. This is equivalent to the threshold we were talking before, just a little rearranged. It can be thought as how difficult is to trigger the neuron. . There are many activation functions. The most popular are the following: . Sigmoid function: | . $$ sigma(x) = frac{1}{1+e^{-x}} $$ . Rectifier function (sometimes called ReLU): | . $$ sigma(x) = max(0,x) $$ . And their shape: . #collapse-hide import numpy as np import matplotlib.pyplot as plt # Define x grid. xx = np.linspace(-4,4,100) # Define response from the different activation functions. y_sig = 1/(1+np.exp(-xx)) y_relu = [max(0,x) for x in xx] # Plot the activation functions. plt.figure(figsize=(5,2)) # Sigmoid ax1 = plt.subplot(121) ax1.plot(xx,y_sig) ax1.set_title(&quot;Sigmoid&quot;) ax1.set_xlabel(&quot;$x$&quot;) ax1.set_ylabel(&quot;$ sigma(x)$&quot;) ax1.grid(alpha=0.3) # ReLU ax2 = plt.subplot(122) ax2.plot(xx,y_relu) ax2.set_title(&quot;ReLU&quot;) ax2.set_xlabel(&quot;$x$&quot;) ax2.set_ylabel(&quot;$ sigma(x)$&quot;) ax2.grid(alpha=0.3) plt.tight_layout(); . . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Neural Networks . Now that we have our neurons, we can build our neural networks by interconnecting them. This would be a simple of example of such networkNow that we have our neurons, we can build our neural networks by interconnecting them. This is simple of example of such network: . . This seems a bit more complicated but it&#39;s actually the same idea with just more neurons. The input layer is basically a representation of the input values (there are no neurons in the input layer). The output layer are the artifical neurons which produce the final outputs. The hidden layer, basically a layer which is not an input nor an output, is constituted by several neurons. Each neuron has its inputs, its outputs and its activation function. If we talk about parameters, each neuron has a set of weights $w$ (one for each input $i$ it receives) and a bias $b$. We can already see that for this very little example we already have a lot of parameters. This will be important later on. . This small &quot;brain&quot; works as following: there are a series of inputs that go to the hidden layer, each neuron in this layer produces an output that becomes an input for the next layer (the output layer in this case) and finally, the output layer neurons produce the final outputs. . Returning to our original problem, we want to implement one of this Neural Networks inside KVN such that when we give him an image of a number as an input, he will then whisper which number it is as an output. . Note: This example is what it is called a feedforward neural network. In these networks, there are no cycles. All information always goes forward from input to output. There are other types of neural networks with different properties, such as recurrent neural networks, but they are more complicated so we will stick to the &quot;easy&quot; case. . So far, we have defined the mini-brain we will implement into KVN. In the following parts, we will explore how Neural Networks can learn a task and, ultimately, do what we want them to do. .",
            "url": "https://bepuca.dev/tutorial/machine%20learning/python/2020/05/24/kvn-part-1.html",
            "relUrl": "/tutorial/machine%20learning/python/2020/05/24/kvn-part-1.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://bepuca.dev/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bepuca.dev/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "You can contact me at bernatpuig [at] gmail [dot] com . I am Bernat and I work at the intersection of data science and software engineering. Mine has been (and it is still is) a journey of beautiful discoveries. I started in physics and eventually transitioned to data science and software development. Today, I strive to embed machine learning into software solutions to help people and organizations alike focus on what matters. I thrive the most when working within diverse and multidisciplinary teams and I believe in the value of Lean and XP software principles. . Curiosity driven, virtually everything interests me. This has exposed me to a myriad of technologies, frameworks and tools and it has also led me into a never-ending quest for learning and personal growth. I always enjoy engaging discussions, so please reach out to me if you want to talk about anything you find interesting. . Also, I like to ski. Ah, I love skiing. .",
          "url": "https://bepuca.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bepuca.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}